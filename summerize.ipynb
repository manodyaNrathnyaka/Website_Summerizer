{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906dd4be-e9c2-4072-8d92-1f0a9b884604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505c1126-84c3-49ed-b80e-f5d6d7348f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e4e2fe-b60f-4214-b52d-06877a1a6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a15383-035f-4589-9e7a-03eb52cd86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api is found\n"
     ]
    }
   ],
   "source": [
    "api_key=os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"No api key was found\")\n",
    "else:\n",
    "    print(\"api is found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f412c3e8-357e-47ab-b474-6658ca1e62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7282f81e-2696-455e-94fb-9d4dcfe6e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds exciting! What kind of project are you working on with LLM (Large Language Models)? Are you developing a chatbot, content generation tool, language translation system, or something else? Let me know how I can assist you!\n"
     ]
    }
   ],
   "source": [
    "message=\"hello,im creating a project using llm models.\"\n",
    "response=openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9ee422-72c1-4461-9f79-78a2ac7499ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e6d647-0458-44e7-8151-9b7ba486c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__ (self,url):\n",
    "        self.url=url\n",
    "        response=requests.get(url,headers=headers)\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        self.title=soup.title.string if soup.title else \"no title found\"\n",
    "        for irrelevent in soup.body ([\"script\",\"img\",\"style\",\"input\"]):\n",
    "            irrelevent.decompose()\n",
    "        self.text= soup.body.get_text(separator=\"\\n\", strip = True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05fde3be-581d-46e1-959b-60d94ed2bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Explorables | PAIR\n",
      "Guidebook\n",
      "Explorables\n",
      "Tools\n",
      "Research\n",
      "Events\n",
      "AI Explorables\n",
      "Big ideas in machine learning, simply explained\n",
      "The rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\n",
      "This ongoing series of interactive essays will walk you through these important concepts.\n",
      "Can Large Language Models Explain Their Internal Mechanisms?\n",
      "An introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\n",
      "Start exploring\n",
      "Do Machine Learning Models Memorize or Generalize?\n",
      "An introduction to grokking and mechanistic interpretability.\n",
      "Start exploring\n",
      "What Have Language Models Learned?\n",
      "By asking language models to fill in the blank, we can probe their understanding of the world.\n",
      "Start exploring\n",
      "Confidently Incorrect Models to Humble Ensembles\n",
      "ML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\n",
      "Start exploring\n",
      "Collecting Sensitive Information\n",
      "The availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\n",
      "Start exploring\n",
      "Why Some Models Leak Data\n",
      "Machine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\n",
      "Start exploring\n",
      "Can a Model Be Differentially Private and Fair?\n",
      "Training models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\n",
      "Start exploring\n",
      "Federated Learning\n",
      "Most machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\n",
      "Start exploring\n",
      "Are Model Predictions Probabilities?\n",
      "Machine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\n",
      "Start exploring\n",
      "Searching for Unintended Biases With Saliency\n",
      "Machine learning models sometimes learn from spurious correlations in training data. Trying to understand\n",
      "how\n",
      "models make predictions gives us a shot at spotting flawed models.\n",
      "Start exploring\n",
      "Datasets Have Worldviews\n",
      "Every dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\n",
      "Start exploring\n",
      "Measuring Diversity\n",
      "Search results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\n",
      "Start exploring\n",
      "Measuring Fairness\n",
      "There are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\n",
      "Start exploring\n",
      "Hidden Bias\n",
      "Models trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\n",
      "Start exploring\n",
      "Google Research\n",
      "Google Design\n"
     ]
    }
   ],
   "source": [
    "ed = Website(\"https://pair.withgoogle.com/explorables/\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf651633-95ed-405c-8e34-5f872a09f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c8ca4de-89dd-4a51-8660-4a056aaaaf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled AI Explorables | PAIR\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Guidebook\n",
      "Explorables\n",
      "Tools\n",
      "Research\n",
      "Events\n",
      "AI Explorables\n",
      "Big ideas in machine learning, simply explained\n",
      "The rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\n",
      "This ongoing series of interactive essays will walk you through these important concepts.\n",
      "Can Large Language Models Explain Their Internal Mechanisms?\n",
      "An introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\n",
      "Start exploring\n",
      "Do Machine Learning Models Memorize or Generalize?\n",
      "An introduction to grokking and mechanistic interpretability.\n",
      "Start exploring\n",
      "What Have Language Models Learned?\n",
      "By asking language models to fill in the blank, we can probe their understanding of the world.\n",
      "Start exploring\n",
      "Confidently Incorrect Models to Humble Ensembles\n",
      "ML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\n",
      "Start exploring\n",
      "Collecting Sensitive Information\n",
      "The availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\n",
      "Start exploring\n",
      "Why Some Models Leak Data\n",
      "Machine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\n",
      "Start exploring\n",
      "Can a Model Be Differentially Private and Fair?\n",
      "Training models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\n",
      "Start exploring\n",
      "Federated Learning\n",
      "Most machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\n",
      "Start exploring\n",
      "Are Model Predictions Probabilities?\n",
      "Machine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\n",
      "Start exploring\n",
      "Searching for Unintended Biases With Saliency\n",
      "Machine learning models sometimes learn from spurious correlations in training data. Trying to understand\n",
      "how\n",
      "models make predictions gives us a shot at spotting flawed models.\n",
      "Start exploring\n",
      "Datasets Have Worldviews\n",
      "Every dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\n",
      "Start exploring\n",
      "Measuring Diversity\n",
      "Search results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\n",
      "Start exploring\n",
      "Measuring Fairness\n",
      "There are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\n",
      "Start exploring\n",
      "Hidden Bias\n",
      "Models trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\n",
      "Start exploring\n",
      "Google Research\n",
      "Google Design\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf9c8d9-3baf-4fc7-be7b-e1418c81b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return[{\"role\":\"user\",\"content\":user_prompt_for(website)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dead1f8a-0c2b-4b76-8bf4-f7d06aced375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'You are looking at a website titled AI Explorables | PAIR\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\nGuidebook\\nExplorables\\nTools\\nResearch\\nEvents\\nAI Explorables\\nBig ideas in machine learning, simply explained\\nThe rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\\nThis ongoing series of interactive essays will walk you through these important concepts.\\nCan Large Language Models Explain Their Internal Mechanisms?\\nAn introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\\nStart exploring\\nDo Machine Learning Models Memorize or Generalize?\\nAn introduction to grokking and mechanistic interpretability.\\nStart exploring\\nWhat Have Language Models Learned?\\nBy asking language models to fill in the blank, we can probe their understanding of the world.\\nStart exploring\\nConfidently Incorrect Models to Humble Ensembles\\nML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\\nStart exploring\\nCollecting Sensitive Information\\nThe availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\\nStart exploring\\nWhy Some Models Leak Data\\nMachine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\\nStart exploring\\nCan a Model Be Differentially Private and Fair?\\nTraining models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\\nStart exploring\\nFederated Learning\\nMost machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\\nStart exploring\\nAre Model Predictions Probabilities?\\nMachine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\\nStart exploring\\nSearching for Unintended Biases With Saliency\\nMachine learning models sometimes learn from spurious correlations in training data. Trying to understand\\nhow\\nmodels make predictions gives us a shot at spotting flawed models.\\nStart exploring\\nDatasets Have Worldviews\\nEvery dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\\nStart exploring\\nMeasuring Diversity\\nSearch results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\\nStart exploring\\nMeasuring Fairness\\nThere are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\\nStart exploring\\nHidden Bias\\nModels trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\\nStart exploring\\nGoogle Research\\nGoogle Design'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbbb7696-7aaf-43aa-b723-a91a930ab7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerizer(url):\n",
    "    website= Website(url)\n",
    "    response=openai.chat.completions.create(\n",
    "        model =\"gpt-4o-mini\",\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484491f1-2edc-4785-baf8-39b5363b7ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# AI Explorables | PAIR\\n\\n**Website Summary**  \\nAI Explorables is a platform that aims to simplify and explain complex concepts in machine learning through interactive essays. It addresses critical questions about fairness, privacy, and the behavior of machine learning models, providing users with insights into contemporary challenges in AI. The site features a series of explorables that delve into topics such as:\\n\\n- **Model Interpretability**: Understanding how models explain their predictions and internal mechanisms.\\n- **Learning Dynamics**: Distinguishing between memorization and generalization in models.\\n- **Data Privacy**: Examining the implications of using large datasets and the risks of data leakage.\\n- **Model Performance**: Investigating the accuracy of model predictions and potential biases.\\n\\nThe content is designed to enhance understanding of these significant issues and is suitable for those looking to explore the implications of AI in society.\\n\\n### Notable Explorables\\n1. **Can Large Language Models Explain Their Internal Mechanisms?**\\n2. **Do Machine Learning Models Memorize or Generalize?**\\n3. **What Have Language Models Learned?**\\n4. **Confidently Incorrect Models to Humble Ensembles**\\n5. **Collecting Sensitive Information**\\n6. **Why Some Models Leak Data**\\n7. **Can a Model Be Differentially Private and Fair?**\\n8. **Federated Learning**\\n9. **Are Model Predictions Probabilities?**\\n10. **Searching for Unintended Biases With Saliency**\\n11. **Datasets Have Worldviews**\\n12. **Measuring Diversity**\\n13. **Measuring Fairness**\\n14. **Hidden Bias**\\n\\n**Research and Tools**  \\nIn addition to explorables, the website includes sections on tools, research, and events related to AI and machine learning, enhancing the users' ability to understand and engage with these topics in greater depth. \\n\\n**No News or Announcements**  \\nThe site does not appear to feature any specific news or announcements at this time.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summerizer(\"https://pair.withgoogle.com/explorables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc97c2b-ed96-4271-aa72-807de51cb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary= summerizer(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ceaca4c-75ce-4a91-af5f-a0b84af41b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI Explorables | PAIR Summary\n",
       "\n",
       "**Overview**  \n",
       "AI Explorables is an initiative that aims to demystify complex concepts in machine learning through a series of interactive essays. The website addresses critical questions about model fairness, interpretable predictions, privacy implications, and the overall impact of machine learning technologies.\n",
       "\n",
       "**Key Sections**  \n",
       "- **Guidebook**: Provides foundational knowledge for understanding machine learning concepts.\n",
       "- **Explorables**: Offers interactive essays that explore various themes and ideas in machine learning, encouraging user engagement and exploration.\n",
       "- **Tools**: Presents software and resources to assist in machine learning projects.\n",
       "- **Research**: Details ongoing research in the field of artificial intelligence.\n",
       "- **Events**: Lists upcoming events related to AI and machine learning.\n",
       "\n",
       "**Highlighted Explorables**  \n",
       "1. **Can Large Language Models Explain Their Internal Mechanisms?** - Introduces Patchscopes for inspecting LLMs' hidden representations.\n",
       "2. **Do Machine Learning Models Memorize or Generalize?** - Discusses grokking and the interpretability of models.\n",
       "3. **What Have Language Models Learned?** - Explores language models' understanding through fill-in-the-blank tasks.\n",
       "4. **Confidently Incorrect Models to Humble Ensembles** - Examines how ensemble models improve prediction reliability.\n",
       "5. **Collecting Sensitive Information** - Addresses privacy concerns related to data collection.\n",
       "6. **Why Some Models Leak Data** - Discusses the risks of inadvertently revealing sensitive data through model training.\n",
       "7. **Can a Model Be Differentially Private and Fair?** - Explores the tension between privacy and accuracy in diverse groups.\n",
       "\n",
       "**Additional Themes**  \n",
       "- Federated Learning: A method to train models without exposing users' raw data.\n",
       "- Model Predictions as Probabilities: Techniques for transforming model scores into probabilities for decision-making.\n",
       "- Searching for Unintended Biases, Measuring Diversity, and Measuring Fairness: Topics concerning biases inherent in datasets and models.\n",
       "\n",
       "Overall, AI Explorables serves as a valuable resource for individuals wanting to understand the implications and intricacies of machine learning in a more accessible manner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://pair.withgoogle.com/explorables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fde2bb-9630-421c-8286-5c11e457982c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
