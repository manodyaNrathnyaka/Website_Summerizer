{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906dd4be-e9c2-4072-8d92-1f0a9b884604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505c1126-84c3-49ed-b80e-f5d6d7348f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e4e2fe-b60f-4214-b52d-06877a1a6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a15383-035f-4589-9e7a-03eb52cd86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api is found\n"
     ]
    }
   ],
   "source": [
    "api_key=os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"No api key was found\")\n",
    "else:\n",
    "    print(\"api is found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f412c3e8-357e-47ab-b474-6658ca1e62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7282f81e-2696-455e-94fb-9d4dcfe6e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds exciting! Large Language Models (LLMs) have a lot of potential applications. What specific project are you working on, and how can I assist you with it?\n"
     ]
    }
   ],
   "source": [
    "message=\"hello,im creating a project using llm models.\"\n",
    "response=openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd9ee422-72c1-4461-9f79-78a2ac7499ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e6d647-0458-44e7-8151-9b7ba486c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__ (self,url):\n",
    "        self.url=url\n",
    "        response=requests.get(url,headers=headers)\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        self.title=soup.title.string if soup.title else \"no title found\"\n",
    "        for irrelevent in soup.body ([\"script\",\"img\",\"style\",\"input\"]):\n",
    "            irrelevent.decompose()\n",
    "        self.text= soup.body.get_text(separator=\"\\n\", strip = True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05fde3be-581d-46e1-959b-60d94ed2bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Explorables | PAIR\n",
      "Guidebook\n",
      "Explorables\n",
      "Tools\n",
      "Research\n",
      "Events\n",
      "AI Explorables\n",
      "Big ideas in machine learning, simply explained\n",
      "The rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\n",
      "This ongoing series of interactive essays will walk you through these important concepts.\n",
      "Can Large Language Models Explain Their Internal Mechanisms?\n",
      "An introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\n",
      "Start exploring\n",
      "Do Machine Learning Models Memorize or Generalize?\n",
      "An introduction to grokking and mechanistic interpretability.\n",
      "Start exploring\n",
      "What Have Language Models Learned?\n",
      "By asking language models to fill in the blank, we can probe their understanding of the world.\n",
      "Start exploring\n",
      "Confidently Incorrect Models to Humble Ensembles\n",
      "ML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\n",
      "Start exploring\n",
      "Collecting Sensitive Information\n",
      "The availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\n",
      "Start exploring\n",
      "Why Some Models Leak Data\n",
      "Machine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\n",
      "Start exploring\n",
      "Can a Model Be Differentially Private and Fair?\n",
      "Training models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\n",
      "Start exploring\n",
      "Federated Learning\n",
      "Most machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\n",
      "Start exploring\n",
      "Are Model Predictions Probabilities?\n",
      "Machine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\n",
      "Start exploring\n",
      "Searching for Unintended Biases With Saliency\n",
      "Machine learning models sometimes learn from spurious correlations in training data. Trying to understand\n",
      "how\n",
      "models make predictions gives us a shot at spotting flawed models.\n",
      "Start exploring\n",
      "Datasets Have Worldviews\n",
      "Every dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\n",
      "Start exploring\n",
      "Measuring Diversity\n",
      "Search results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\n",
      "Start exploring\n",
      "Measuring Fairness\n",
      "There are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\n",
      "Start exploring\n",
      "Hidden Bias\n",
      "Models trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\n",
      "Start exploring\n",
      "Google Research\n",
      "Google Design\n"
     ]
    }
   ],
   "source": [
    "ed = Website(\"https://pair.withgoogle.com/explorables/\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf651633-95ed-405c-8e34-5f872a09f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c8ca4de-89dd-4a51-8660-4a056aaaaf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled AI Explorables | PAIR\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Guidebook\n",
      "Explorables\n",
      "Tools\n",
      "Research\n",
      "Events\n",
      "AI Explorables\n",
      "Big ideas in machine learning, simply explained\n",
      "The rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\n",
      "This ongoing series of interactive essays will walk you through these important concepts.\n",
      "Can Large Language Models Explain Their Internal Mechanisms?\n",
      "An introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\n",
      "Start exploring\n",
      "Do Machine Learning Models Memorize or Generalize?\n",
      "An introduction to grokking and mechanistic interpretability.\n",
      "Start exploring\n",
      "What Have Language Models Learned?\n",
      "By asking language models to fill in the blank, we can probe their understanding of the world.\n",
      "Start exploring\n",
      "Confidently Incorrect Models to Humble Ensembles\n",
      "ML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\n",
      "Start exploring\n",
      "Collecting Sensitive Information\n",
      "The availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\n",
      "Start exploring\n",
      "Why Some Models Leak Data\n",
      "Machine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\n",
      "Start exploring\n",
      "Can a Model Be Differentially Private and Fair?\n",
      "Training models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\n",
      "Start exploring\n",
      "Federated Learning\n",
      "Most machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\n",
      "Start exploring\n",
      "Are Model Predictions Probabilities?\n",
      "Machine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\n",
      "Start exploring\n",
      "Searching for Unintended Biases With Saliency\n",
      "Machine learning models sometimes learn from spurious correlations in training data. Trying to understand\n",
      "how\n",
      "models make predictions gives us a shot at spotting flawed models.\n",
      "Start exploring\n",
      "Datasets Have Worldviews\n",
      "Every dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\n",
      "Start exploring\n",
      "Measuring Diversity\n",
      "Search results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\n",
      "Start exploring\n",
      "Measuring Fairness\n",
      "There are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\n",
      "Start exploring\n",
      "Hidden Bias\n",
      "Models trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\n",
      "Start exploring\n",
      "Google Research\n",
      "Google Design\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccf9c8d9-3baf-4fc7-be7b-e1418c81b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return[{\"role\":\"user\",\"content\":user_prompt_for(website)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dead1f8a-0c2b-4b76-8bf4-f7d06aced375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'You are looking at a website titled AI Explorables | PAIR\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\nGuidebook\\nExplorables\\nTools\\nResearch\\nEvents\\nAI Explorables\\nBig ideas in machine learning, simply explained\\nThe rapidly increasing usage of machine learning raises complicated questions: How can we tell if models are fair? Why do models make the predictions that they do? What are the privacy implications of feeding enormous amounts of data into models?\\nThis ongoing series of interactive essays will walk you through these important concepts.\\nCan Large Language Models Explain Their Internal Mechanisms?\\nAn introduction to Patchscopes, an inspection framework for explaining the hidden representations of LLMs, with LLMs.\\nStart exploring\\nDo Machine Learning Models Memorize or Generalize?\\nAn introduction to grokking and mechanistic interpretability.\\nStart exploring\\nWhat Have Language Models Learned?\\nBy asking language models to fill in the blank, we can probe their understanding of the world.\\nStart exploring\\nConfidently Incorrect Models to Humble Ensembles\\nML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes.\\nStart exploring\\nCollecting Sensitive Information\\nThe availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people’s privacy.\\nStart exploring\\nWhy Some Models Leak Data\\nMachine learning models use large amounts of data, some of which can be sensitive. If they’re not trained correctly, sometimes that data is inadvertently revealed.\\nStart exploring\\nCan a Model Be Differentially Private and Fair?\\nTraining models with differential privacy stops models from inadvertently leaking sensitive data, but there’s an unexpected side-effect: reduced accuracy on underrepresented subgroups.\\nStart exploring\\nFederated Learning\\nMost machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user’s raw data leaving their device.\\nStart exploring\\nAre Model Predictions Probabilities?\\nMachine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making.\\nStart exploring\\nSearching for Unintended Biases With Saliency\\nMachine learning models sometimes learn from spurious correlations in training data. Trying to understand\\nhow\\nmodels make predictions gives us a shot at spotting flawed models.\\nStart exploring\\nDatasets Have Worldviews\\nEvery dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too.\\nStart exploring\\nMeasuring Diversity\\nSearch results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help.\\nStart exploring\\nMeasuring Fairness\\nThere are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people.\\nStart exploring\\nHidden Bias\\nModels trained on real-world data can encode real-world bias. Hiding information about protected classes doesn’t always fix things — sometimes it can even hurt.\\nStart exploring\\nGoogle Research\\nGoogle Design'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbbb7696-7aaf-43aa-b723-a91a930ab7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerizer(url):\n",
    "    website= Website(url)\n",
    "    response=openai.chat.completions.create(\n",
    "        model =\"gpt-4o-mini\",\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "484491f1-2edc-4785-baf8-39b5363b7ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# AI Explorables | PAIR\\n\\n**Overview:**  \\nAI Explorables is an educational platform that simplifies complex concepts in machine learning, focusing on big ideas and important ethical considerations. The website offers an ongoing series of interactive essays designed to enhance understanding of various topics, such as model fairness, interpretation, privacy implications, and more.\\n\\n## Key Sections:\\n\\n- **Guidebook:**  \\n  A resource for navigating the world of machine learning concepts.\\n\\n- **Explorables:**  \\n  Interactive essays on significant machine learning ideas, including:\\n  - **Patchscopes:** An inspection framework for large language models.\\n  - **Grokking and Mechanistic Interpretability:** Exploring the balance between memorization and generalization in models.\\n  - **Understanding Language Models:** Investigating what language models have truly learned.\\n  - **Confidence in Predictions:** The challenges of models making incorrect predictions.\\n  - **Privacy Concerns:** Discussing the challenges of data collection and privacy violations.\\n  - **Data Leakage:** Investigating how models can inadvertently reveal sensitive data.\\n  - **Differential Privacy vs. Fairness:** Balancing the privacy of sensitive data with representation accuracy in models.\\n  - **Federated Learning:** A method that allows training models without raw user data.\\n  - **Probabilities in Predictions:** Transforming model scores into true probabilities for better decision making.\\n  - **Bias Detection:** Methods for identifying unwarranted biases in model predictions.\\n  - **Dataset Perspectives:** Understanding how datasets shape conclusions based on their inherent biases.\\n  - **Measuring Diversity and Fairness:** Examining the importance of diversity in datasets to address stereotypes and inequity.\\n  - **Hidden Bias in Real-World Data:** How training data can embed real-world biases.\\n\\n- **Tools:**  \\n  Resources and tools for exploring machine learning concepts in practice.\\n\\n- **Research:**  \\n  A section dedicated to ongoing academic efforts in machine learning and AI ethics.\\n\\n- **Events:**  \\n  Information about upcoming events, workshops, or conferences related to AI and machine learning.\\n\\n## Summary of Announcements:  \\nCurrently, there are no specific news or announcements highlighted on the site. The emphasis remains on providing educational content and resources related to machine learning concepts.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summerizer(\"https://pair.withgoogle.com/explorables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bc97c2b-ed96-4271-aa72-807de51cb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary= summerizer(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ceaca4c-75ce-4a91-af5f-a0b84af41b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI Explorables | PAIR Summary\n",
       "\n",
       "**Website Overview:**\n",
       "AI Explorables is an educational platform dedicated to simplifying complex concepts in machine learning. It addresses critical questions surrounding fairness, interpretability, and privacy as machine learning technology becomes increasingly prevalent. The site features a series of interactive essays exploring various topics to enhance understanding.\n",
       "\n",
       "## Key Sections\n",
       "- **Guidebook:** Resourceful links and guidelines for navigating machine learning concepts.\n",
       "- **Explorables:** Interactive essays on machine learning topics.\n",
       "- **Tools:** Utilities and applications relevant to machine learning.\n",
       "- **Research:** Insights and findings from ongoing studies in AI.\n",
       "- **Events:** Information about upcoming conferences and meeting opportunities in the field of AI.\n",
       "\n",
       "## Featured Explorables\n",
       "- **Can Large Language Models Explain Their Internal Mechanisms?**  \n",
       "  Introduction to Patchscopes, a framework for explaining the hidden aspects of LLMs.\n",
       "\n",
       "- **Do Machine Learning Models Memorize or Generalize?**  \n",
       "  Examines the concepts of grokking and mechanistic interpretability.\n",
       "\n",
       "- **What Have Language Models Learned?**  \n",
       "  Investigates language models' understanding by analyzing their predictions.\n",
       "\n",
       "- **Confidently Incorrect Models to Humble Ensembles:**  \n",
       "  Discusses how ensembles can improve prediction accuracy by averaging mistakes.\n",
       "\n",
       "- **Collecting Sensitive Information:**  \n",
       "  Highlights challenges in privacy due to vast datasets and powerful computing.\n",
       "\n",
       "- **Why Some Models Leak Data:**  \n",
       "  Explains the risks of sensitive data exposure in incorrectly trained models.\n",
       "\n",
       "- **Can a Model Be Differentially Private and Fair?**  \n",
       "  Analyzes the trade-offs between privacy and accuracy in underrepresented groups.\n",
       "\n",
       "- **Federated Learning:**  \n",
       "  Discusses training models while keeping raw user data secure on devices.\n",
       "\n",
       "- **Are Model Predictions Probabilities?**  \n",
       "  Focuses on calibrating model scores into probabilities for better decision-making.\n",
       "\n",
       "- **Searching for Unintended Biases With Saliency:**  \n",
       "  Explores methods to uncover biases learned from flawed training data.\n",
       "\n",
       "- **Datasets Have Worldviews:**  \n",
       "  Examines how different datasets can lead to varying conclusions.\n",
       "\n",
       "- **Measuring Diversity:**  \n",
       "  Addresses the impact of historic inequities in data representation.\n",
       "\n",
       "- **Measuring Fairness:**  \n",
       "  Discusses the variability of model accuracy across different demographic groups.\n",
       "\n",
       "- **Hidden Bias:**  \n",
       "  Explores how real-world bias can persist in models, even when attempts are made to conceal it.\n",
       "\n",
       "**Google Research and Google Design** are also associated with this platform, contributing insights and collaborative efforts.\n",
       "\n",
       "This web platform serves as both an educational resource and a tool for furthering discussions on ethical and practical implications of AI and machine learning technologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://pair.withgoogle.com/explorables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fde2bb-9630-421c-8286-5c11e457982c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
